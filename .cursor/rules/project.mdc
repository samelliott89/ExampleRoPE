---
description: Project context for ExampleRoPE transformer implementation
globs: ["**/*.py"]
---

# ExampleRoPE - Transformer Implementation Tutorial

## Project Goal
Building a minimal transformer from scratch with modern techniques (LLaMA-style), focusing on understanding each component.

## Current State - COMPLETE

### Files
- `model.py` - Full transformer with training loop
- `data.py` - Tiny Shakespeare dataset + character tokenizer
- `optimizer.py` - Muon + AdamW optimizers

### Completed Components

**Config** - Centralized hyperparameters:
- `dim=128`, `hidden_dim=512`, `n_layers=4`
- `n_heads=8`, `n_kv_heads=4` (GQA with 2 query heads per KV head)
- `max_seq_len=256`, `vocab_size=65` (set from tokenizer)
- Training: `batch_size=32`, `max_iters=5000`, `learning_rate=3e-4`

**RoPE** - Rotary Position Embeddings:
- `precompute_freqs_cis` - precomputes rotation frequencies
- `apply_rotary_emb` - applies rotation to Q, K via complex multiplication

**RMSNorm** - Root Mean Square Layer Normalization

**FeedForward** - SwiGLU MLP (w1, w2, w3, no bias)

**Attention** - Grouped Query Attention with RoPE

**TransformerBlock** - Pre-norm architecture: norm → attn → residual → norm → ffn → residual

**Transformer** - Full model with tok_embeddings, layers, output projection

**Training**:
- `train()` - full training loop with lr warmup/decay
- `estimate_loss()` - evaluate on train/val
- `generate()` - autoregressive text generation

**Data** (`data.py`):
- Downloads Tiny Shakespeare (~1MB)
- Character-level tokenizer (65 chars)
- 90/10 train/val split
- `TextDataset.get_batch()` for random sequence sampling

## Design Patterns

1. **Config dataclass** - all hyperparameters in one place
2. **No bias** on linear layers (LLaMA convention)
3. **Pre-norm** architecture (norm before attention/ffn)
4. **Cosine LR decay** with warmup

## Running

```bash
python model.py  # trains and generates sample
python data.py   # test data loading
```

## Possible Extensions
- KV Cache for faster inference
- Gradient checkpointing for longer sequences
- Mixed precision training (fp16/bf16)
- Save/load checkpoints
